{"cells":[{"cell_type":"markdown","metadata":{"id":"ix859rgxHVf5"},"source":["# **Crawling Data (Tugas 1)**"]},{"cell_type":"markdown","source":["Crawling data adalah proses mengumpulkan data secara otomatis dari berbagai sumber di internet dengan menggunakan program yang disebut \"web crawler\" atau \"spider\". Web crawler adalah program yang dirancang untuk mengunjungi situs web dan mengumpulkan informasi yang berguna, seperti teks, gambar, atau link. Data yang dikumpulkan oleh web crawler kemudian dapat dianalisis dan digunakan untuk berbagai keperluan seperti riset pasar, pemantauan harga, analisis sentimen, dan sebagainya. Namun, penting untuk diingat bahwa pengumpulan data secara otomatis dapat melanggar hak cipta atau privasi, sehingga perlu memperhatikan etika dan hukum dalam penggunaannya."],"metadata":{"id":"cMcrvvPu5yEy"}},{"cell_type":"markdown","metadata":{"id":"Z2z4noyyAdga"},"source":["## Import Library"]},{"cell_type":"markdown","source":["Library yang dibutuhan antara lain:\n","\n","1. BeautifulSoup = library untuk mengcrawling data berita\n","2. snscrape = library untuk mengcrawling data twitter\n","3. pandas\n","4. CSV"],"metadata":{"id":"zfVPGFlrZFRp"}},{"cell_type":"code","source":["%pip install snscrape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wezERsMz40sv","executionInfo":{"status":"ok","timestamp":1677111970787,"user_tz":-420,"elapsed":4142,"user":{"displayName":"20-086 Moch. Rizki Aji S.","userId":"12494656249188556694"}},"outputId":"c0d1c297-1874-4b53-8431-750e7ca8a333"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: snscrape in /usr/local/lib/python3.8/dist-packages (0.5.0.20230113)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from snscrape) (3.9.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from snscrape) (2.25.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.6.3)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from snscrape) (2022.7.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.9.2)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2.10)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.7.1)\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Q_aP7WbZLy-1","executionInfo":{"status":"ok","timestamp":1677111970788,"user_tz":-420,"elapsed":20,"user":{"displayName":"20-086 Moch. Rizki Aji S.","userId":"12494656249188556694"}}},"outputs":[],"source":["import requests as req\n","from bs4 import BeautifulSoup as bs\n","from datetime import datetime\n","import snscrape.modules.twitter as sntwitter\n","import pandas as pd\n","import csv\n","\n","userAgent = {'user-agent':'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Mobile Safari/537.36'}"]},{"cell_type":"markdown","metadata":{"id":"nzTu4k7DBDA9"},"source":["##Crawling data di twitter"]},{"cell_type":"markdown","source":["Snscrape adalah sebuah alat yang digunakan untuk melakukan crawling data dari media sosial Twitter. Snscrape memiliki kemampuan untuk mengambil data tweet, pengguna, hashtag, dan topik yang populer pada Twitter.\n","\n","Untuk menggunakan Snscrape, Anda harus menginstal Python terlebih dahulu. Setelah itu, Anda dapat menginstal Snscrape dengan perintah \"pip install snscrape\" di terminal. Kemudian, untuk melakukan crawling data Twitter, Anda dapat menjalankan perintah Snscrape di terminal dengan argumen yang sesuai dengan kebutuhan Anda."],"metadata":{"id":"Rt9nLQAf7XYs"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"Gd_yyue7BWiX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677112034721,"user_tz":-420,"elapsed":63952,"user":{"displayName":"20-086 Moch. Rizki Aji S.","userId":"12494656249188556694"}},"outputId":"931a946a-2049-4e28-96ae-c4aa70069829"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-d6ab54e2233a>:10: FutureWarning: content is deprecated, use rawContent instead\n","  wr.writerow([tweet.date, tweet.user.username, tweet.content])\n"]}],"source":["from textblob import TextBlob\n","import re\n","\n","# tweets = []\n","for i,tweet in enumerate(sntwitter.TwitterSearchScraper('Anies Baswedan Calon Presiden since:2022-01-01 until:2023-02-20').get_items()):\n","    if i > 1500:\n","        break\n","    with open('data-twitter.csv', 'a') as file:\n","      wr = csv.writer(file, delimiter=',')\n","      wr.writerow([tweet.date, tweet.user.username, tweet.content])\n","# tweets_df = pd.DataFrame(tweets, columns=['Datetime', 'Username Twitter', 'Tweet'])\n"]},{"cell_type":"markdown","metadata":{"id":"jI9SsHbQAwIh"},"source":["##Crawliing Jurnal di pta-trunojoyo.ac.id"]},{"cell_type":"markdown","source":["Crawling data web dengan Beautiful Soup adalah teknik untuk mengekstrak data dari halaman web dengan menggunakan library Python yang bernama Beautiful Soup. Beautiful Soup menyediakan cara yang mudah untuk menavigasi struktur HTML dan XML dari halaman web, sehingga memungkinkan kita untuk mengambil data yang kita butuhkan.\n","\n","Pertama-tama, kita harus memuat halaman web yang ingin kita ambil datanya menggunakan library Python seperti requests. Setelah itu, kita dapat menggunakan Beautiful Soup untuk melakukan parsing halaman web tersebut dan mengambil data yang kita butuhkan, seperti teks, gambar, link, dan sebagainya. Beautiful Soup juga dapat digunakan untuk melakukan filtering data berdasarkan kriteria tertentu, seperti tag HTML atau atributnya.\n","\n","Dalam praktiknya, proses crawling data web dengan Beautiful Soup melibatkan beberapa tahapan seperti mengakses URL, memuat halaman web, melakukan parsing, mengambil data, dan menyimpannya ke dalam format yang sesuai. Meskipun teknik ini relatif mudah dipelajari, namun penting untuk diingat bahwa crawling data web juga memiliki beberapa risiko dan batasan terkait dengan etika, keamanan, dan privasi data."],"metadata":{"id":"ZF_8x-ac8p3q"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"aupmqcBEh---","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677112224346,"user_tz":-420,"elapsed":189683,"user":{"displayName":"20-086 Moch. Rizki Aji S.","userId":"12494656249188556694"}},"outputId":"0e67df0f-ebd6-46a6-b70e-0efb126b2966"},"outputs":[{"output_type":"stream","name":"stdout","text":["done[1] >> 110411100064\n","done[2] >> 110411100097\n","done[3] >> 130411100064\n","done[4] >> 120411100107\n","done[5] >> 100411100060\n","done[6] >> 120411100105\n","done[7] >> 110411100045\n","done[8] >> 140411100073\n","done[9] >> 140411100128\n","done[10] >> 140411100050\n","done[11] >> 140411100024\n","done[12] >> 140411100022\n","done[13] >> 140411100102\n","done[14] >> 110411100019\n","done[15] >> 110411100040\n","done[16] >> 140411100071\n","done[17] >> 140411100092\n","done[18] >> 130411100008\n","done[19] >> 140411100100\n","done[20] >> 140411100091\n","done[21] >> 140411100074\n","done[22] >> 140411100084\n","done[23] >> 120411100022\n","done[24] >> 140411100130\n","done[25] >> 140411100119\n","done[26] >> 140411100142\n","done[27] >> 140411100052\n","done[28] >> 120411100090\n","done[29] >> 140411100098\n","done[30] >> 140411100060\n","done[31] >> 140411100093\n","done[32] >> 120411100018\n","done[33] >> 140411100075\n","done[34] >> 140411100056\n","done[35] >> 140411100029\n","done[36] >> 140411100140\n","done[37] >> 140411100094\n","done[38] >> 130411100015\n","done[39] >> 140411100097\n","done[40] >> 130411100006\n","done[41] >> 130411100010\n","done[42] >> 130411100091\n","done[43] >> 130411100109\n","done[44] >> 110411100007\n","done[45] >> 120411100007\n","done[46] >> 140411100116\n","done[47] >> 140411200150\n","done[48] >> 140411100104\n","done[49] >> 120411100040\n","done[50] >> 110411100018\n","done[51] >> 110411100010\n","done[52] >> 110411100056\n","done[53] >> 140411100106\n","done[54] >> 140411100126\n","done[55] >> 140411100141\n","done[56] >> 140411100146\n","done[57] >> 140411100081\n","done[58] >> 140411100063\n","done[59] >> 140411100096\n","done[60] >> 140411100013\n","done[61] >> 140411100054\n","done[62] >> 140411100088\n","done[63] >> 130411100114\n","done[64] >> 140411100041\n","done[65] >> 140411100017\n","done[66] >> 140411100010\n","done[67] >> 140411100111\n","done[68] >> 170411200130\n","done[69] >> 170411200137\n","done[70] >> 140411100087\n","done[71] >> 140411100070\n","done[72] >> 120411100083\n","done[73] >> 140411100069\n","done[74] >> 140411100011\n","done[75] >> 140411100046\n","done[76] >> 140411100080\n","done[77] >> 140411100118\n","done[78] >> 140411100117\n","done[79] >> 140411100027\n","done[80] >> 140411100020\n","done[81] >> 140411100109\n","done[82] >> 120411100050\n","done[83] >> 140411100048\n","done[84] >> 140411200151\n","done[85] >> 140411100051\n","done[86] >> 140411100085\n","done[87] >> 130411100087\n","done[88] >> 140411100061\n","done[89] >> 120411100015\n","done[90] >> 140411100103\n","done[91] >> 130411100106\n","done[92] >> 140411100044\n","done[93] >> 140411100132\n","done[94] >> 130411100098\n","done[95] >> 130411100011\n","done[96] >> 140411100145\n","done[97] >> 120411100066\n","done[98] >> 140411100124\n","done[99] >> 120411100086\n","done[100] >> 150411100016\n","done[101] >> 150411100052\n","done[102] >> 150411100058\n","done[103] >> 150411100051\n","done[104] >> 150411100001\n","done[105] >> 150411100108\n","done[106] >> 150411100106\n","done[107] >> 150411100078\n","done[108] >> 150411100112\n","done[109] >> 150411100115\n","done[110] >> 150411100091\n","done[111] >> 150411100072\n","done[112] >> 150411100066\n","done[113] >> 150411100061\n","done[114] >> 150411100043\n","done[115] >> 150411100088\n","done[116] >> 170411200135\n","done[117] >> 150411100030\n","done[118] >> 150411100045\n","done[119] >> 140411100082\n","done[120] >> 140411100090\n","done[121] >> 140411100016\n","done[122] >> 140411100137\n","done[123] >> 140411100039\n","done[124] >> 140411100025\n","done[125] >> 140411100042\n","done[126] >> 150411100041\n","done[127] >> 170411200134\n","done[128] >> 170411200133\n","done[129] >> 150411100107\n","done[130] >> 140411100057\n","done[131] >> 150411100017\n","done[132] >> 120411100048\n","done[133] >> 150411100096\n","done[134] >> 140411100009\n","done[135] >> 160411200188\n","done[136] >> 170411200129\n","done[137] >> 140411100031\n","done[138] >> 140411100034\n","done[139] >> 140411100008\n","done[140] >> 150411100021\n","done[141] >> 150411100035\n","done[142] >> 120411100072\n","done[143] >> 170411200128\n","done[144] >> 150411100070\n","done[145] >> 140411100078\n","done[146] >> 140411100143\n","done[147] >> 150411100097\n","done[148] >> 130411100093\n","done[149] >> 150411100029\n","done[150] >> 150411100013\n","done[151] >> 150411100024\n","done[152] >> 150411100027\n","done[153] >> 150411100125\n","done[154] >> 150411100068\n","done[155] >> 150411100121\n","done[156] >> 140411100065\n","done[157] >> 140411100099\n","done[158] >> 140411100043\n","done[159] >> 140411100115\n","done[160] >> 150411100033\n","done[161] >> 140411100053\n","done[162] >> 140411100028\n","done[163] >> 150411100042\n","done[164] >> 140411100120\n","done[165] >> 150411100111\n","done[166] >> 150411100055\n","done[167] >> 150411100025\n","done[168] >> 150411100005\n","done[169] >> 150411100019\n","done[170] >> 150411100074\n","done[171] >> 140411100072\n","done[172] >> 140411100032\n","done[173] >> 130411100074\n","done[174] >> 170411200138\n","done[175] >> 150411100004\n","done[176] >> 150411100031\n","done[177] >> 150411100064\n","done[178] >> 130411100082\n","done[179] >> 130411100094\n","done[180] >> 150411100124\n","done[181] >> 130411100046\n","done[182] >> 150411100123\n","done[183] >> 150411100059\n","done[184] >> 150411100104\n","done[185] >> 140411100045\n","done[186] >> 150411100002\n","done[187] >> 130411200123\n","done[188] >> 130411100034\n","done[189] >> 130411100086\n","done[190] >> 130411100069\n","done[191] >> 130411100053\n","done[192] >> 150411100009\n","done[193] >> 150411100040\n","done[194] >> 150411100003\n","done[195] >> 150411100015\n","done[196] >> 150411100082\n","done[197] >> 170411200127\n","done[198] >> 170411200139\n","done[199] >> 170411200126\n","done[200] >> 150411100084\n"]}],"source":["a = 1\n","for page in range(130, 170):\n","  url = f'https://pta.trunojoyo.ac.id/c_search/byprod/10/{page}'\n","  ge = req.get(url,userAgent).text\n","  sop = bs(ge, 'lxml')\n","  pd = sop.find('div',{'id':'begin'}).find('h2').text\n","  prodi = ''.join(pd).replace('Journal Jurusan','')\n","  li = sop.find('ul', class_='items list_style')\n","  lin = li.find_all('li')\n","  for x in lin:\n","    link = x.find('a', class_ ='gray button')['href']\n","    headline = x.find('div').find('a').text\n","    headline_ = ''.join(headline).replace('\\n','')\n","    ge_ = req.get(link,userAgent).text\n","    sop_ = bs(ge_,'lxml')\n","    nim = x.find('div',{'style':'margin: 25px 0px 15px 0px;'}).find('a', class_='gray button')['href']\n","    nim_ = ''.join(nim).replace('https://pta.trunojoyo.ac.id/welcome/detail/','')\n","    content = sop_.find_all('ul', class_='items list_style')\n","    for x in content:\n","      nm = x.find('div',{'style':'padding:2px 2px 2px 2px;'}).find('span').text\n","      nama = ''.join(nm).replace('Penulis : ','')\n","      x = x.find_all('p')\n","      y = [y.text for y in x]\n","      content_ = ''.join(y).replace('\\n','')\n","      print(f'done[{a}] >> {nim_[0:]}')\n","      a += 1\n","      with open('pta-trunojoyo.csv', 'a') as file:\n","        wr = csv.writer(file, delimiter=',')\n","        wr.writerow([nim_,nama,headline,content_,prodi])"]},{"cell_type":"markdown","metadata":{"id":"BoUFpWuUA8wW"},"source":["##Crawling Data Berita Online di detik.com"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8D5CMtn_MvO8","outputId":"0b9cec91-7131-4663-d831-5de2d48d1bbf"},"outputs":[{"output_type":"stream","name":"stdout","text":["done[1] >> Gus Muhaimin Ingin Kebudayaan Jadi Panglima Pembangunan Bangsa\n","done[2] >> Sudirman Said soal Anies Merosot di Survei Kompas: Belum Kampanye Resmi\n","done[3] >> Jabar Hari Ini: Hukuman Lebih Berat untuk Doni Salmanan\n","done[4] >> Sudirman Said: Anies Perlu Waktu dan Ketenangan untuk Tentukan Cawapres\n","done[5] >> Partai Gelora Pede Jabar Jadi Lumbung Suara di Pemilu 2024\n","done[6] >> Anies Dukung Sistem Pemilu Terbuka: Lebih Baik untuk Demokrasi\n","done[7] >> Ketua DPP Golkar Tolak Sistem Coblos Partai: Bikin Lupa Turba ke Rakyat\n","done[8] >> Bupati Kendal Dico Temui Gibran di Solo, Kaget soal Survei Pilgub Jateng\n","done[9] >> Projo Beri Selamat JoMan Dukung Prabowo di Pilpres 2024, Bakal Nyusul?\n","done[10] >> Gus Muhaimin Ingin Kebudayaan Jadi Panglima Pembangunan Bangsa\n","done[11] >> Sudirman Said soal Anies Merosot di Survei Kompas: Belum Kampanye Resmi\n","done[12] >> Jabar Hari Ini: Hukuman Lebih Berat untuk Doni Salmanan\n","done[13] >> Sudirman Said: Anies Perlu Waktu dan Ketenangan untuk Tentukan Cawapres\n","done[14] >> Partai Gelora Pede Jabar Jadi Lumbung Suara di Pemilu 2024\n","done[15] >> Anies Dukung Sistem Pemilu Terbuka: Lebih Baik untuk Demokrasi\n","done[16] >> Ketua DPP Golkar Tolak Sistem Coblos Partai: Bikin Lupa Turba ke Rakyat\n","done[17] >> Bupati Kendal Dico Temui Gibran di Solo, Kaget soal Survei Pilgub Jateng\n","done[18] >> Projo Beri Selamat JoMan Dukung Prabowo di Pilpres 2024, Bakal Nyusul?\n","done[19] >> Demokrat: Kasihan Koalisi Lain, Nama Capresnya Belum Ada\n","done[20] >> Pastikan Pemilih Meninggal, Petugas Pantarlih Luwu Cek Langsung ke Kuburan\n","done[21] >> Duet Anies-Sandi Mencuat di Survei Voxpol, PKS: Siapa Saja Berpeluang\n","done[22] >> Relawan Jokowi Akan Gelar Musra Lagi di 11 Provinsi, Terakhir di Jakarta\n","done[23] >> Besok, PKS Deklarasi Resmi Anies Baswedan Capres 2024!\n","done[24] >> Paloh Cerita Kunjungi PKS Sebelum Berkoalisi, Ungkit Rangkulan Persahabatan\n","done[25] >> Elektabilitas NasDem Melesat Naik, Surya Paloh: Kami Terbiasa Rendah\n","done[26] >> Cak Imin Beber Alasan Koalisi Gerindra-PKB Belum Umumkan Capres-Cawapres\n","done[27] >> Ganjar Unggul Versi Litbang Kompas, PDIP Tetap Tunggu Arahan Megawati\n","done[28] >> Kelakar Surya Paloh soal Cawapres Anies: Jangankan AHY, Saya Juga Cocok\n","done[29] >> Cak Imin Buka Suara soal Potensi Duet dengan Anies di Pilpres 2024\n","done[30] >> Banyak Proyek Hunian Mangkrak, Industri Properti Belum Sembuh dari Pandemi?\n","done[31] >> AHY Dampingi Anies? Paloh: Potongan Ganteng, Lebih dari Pantas\n","done[32] >> Bamsoet: Toleransi dalam Beragama Tak Boleh Hanya Bersifat Retorika\n","done[33] >> Surya Paloh: Sikap Pemerintah Jelas Dukung Sistem Pemilu Terbuka\n","done[34] >> TGB Dukung Gibran Maju Gubernur: Sunatullah...\n","done[35] >> Respons Santai Cak Imin soal Gus Yahya Sebut Warga NU Tak Haram Pilih PAN\n","done[36] >> Kabar Terkini Bocah Kembar Prabowo-Sandiaga yang Sempat Bikin Heboh\n","done[37] >> AHY di Depan Paloh: Semoga Koalisi Perubahan Bisa Berlayar dan Menang\n","done[38] >> AHY: Demokrat dan NasDem Terdepan Tolak Pemilu Coblos Partai!\n","done[39] >> Gerindra soal Survei Litbang Kompas: Kami Semangat Tren Prabowo Stabil\n"]}],"source":["a = 1\n","for page in range(0, 30):\n","  url = f'https://www.detik.com/search/searchall?query=pemilu+2024&sortby=time&page={page}'\n","  ge = req.get(url,userAgent).text\n","  sop = bs(ge, 'lxml')\n","  li = sop.find('div', class_='list media_rows list-berita')\n","  lin = li.find_all('article')\n","  for x in lin:\n","    link = x.find('a')['href']\n","    headline = x.find('a').find('h2').text\n","    ge_ = req.get(link,userAgent).text\n","    sop_ = bs(ge_,'lxml')\n","    content = sop_.find_all('div', class_='detail__body-text itp_bodycontent')\n","    for x in content:\n","      x = x.find_all('p')\n","      y = [y.text for y in x]\n","      content_ = ''.join(y).replace('\\n','').replace('ADVERTISMENT','').replace('SCROLL TO RESUME CONTENT', '')\n","      print(f'done[{a}] >> {headline[0:]}')\n","      a += 1\n","      with open('beritaonline.csv', 'a') as file:\n","        wr = csv.writer(file, delimiter=',')\n","        wr.writerow([headline,content_])"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}